{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "**!¡!¡ Missing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of a model\n",
    "\n",
    "At the beginning it is important to define what actually a \"model\" is. \n",
    "In the lecture it was defined as the following: \n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\"It’s a human-made (abstract) simplification/representation of (observational) reality that is used to understand, define, quantify, visualize, or simulate a part or feature of reality\"\n",
    "</div>\n",
    "\n",
    "\n",
    "An easy example for models is a land map of the world. The following picture shows a picture of the whole world in reality: \n",
    "\n",
    "!¡!¡ Image missing\n",
    "\n",
    "![TheEarth.png](attachment:TheEarth.png)\n",
    "\n",
    "\n",
    "Even if this picture shows the real world, many things cannot be recognized, as for example a large part of the world is hidden by clouds and there are no national borders to recognize.￼￼\n",
    "\n",
    "The following model can be used to recognize the entire country area and the country borders:\n",
    "\n",
    "!¡!¡ Image missing\n",
    "\n",
    "![Worldmodel.png](attachment:Worldmodel.png)\n",
    "\n",
    "Thereby, the model is just a human-made simplification of the real world to visualise clearly the earth's surface, country borders and country names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of determining the distribution of human size by a model\n",
    "\n",
    "In this subchapter an experiment is described in five steps, which was carried out with students of the course. In this experiment, a model describing human distribution was tested with real life data.\n",
    "\n",
    ">Step 1 - Derivation of the model: After the teacher asked about a model which is describing human height, a student answered that human height is determined by a normal distribution. Therefore, the normal distribution was tested.\n",
    "\n",
    "\n",
    ">Step 2 - Execution: To verify this, the data was collected in the following Python [code](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1099891). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [175,170,205,190,190,200,185,190,185,165,170,185,185,180,195,175,190,190,160,180] # List of all heights from the students\n",
    "plt.hist(heights, bins='auto')  # arguments are passed to np.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Used functions and keywords (and etc.): [dictionary](https://docs.python.org/3/tutorial/datastructures.html), [.hist()](https://plot.ly/matplotlib/histograms/)</div>\n",
    "\n",
    ">Step 3 - Result: If you run the written code, then you see the distribution of the size of the students. It becomes clear that the height in the course is not normally distributed. \n",
    "\n",
    ">Step 4 - Interpretations:\n",
    ">>Interpretations of students: \n",
    ">>>a. Sample too small\n",
    "\n",
    ">>>b. Not i.i.d., e.g. only 8 women out of 20 students in total\n",
    "\n",
    ">> Correct answer: Human size is not determined by the normal distribution, but by other factors, such as size of parents, diet, childhood diseases but also still unknown variables, which one can assume as random variables. \n",
    "\n",
    ">Step 5 - Conclusion: The results of models of human height and also of many other models in theory often fit only with those in reality and is not a data generating process that does not generate the size of people by the relevant factors. Because many processes cannot be represented 100% by data transmission processes in reality, this is often not possible because not all factors and data of the generation process are known. There can be many models/human constructs that fit the data of human distribution, but there are only certain data that generate human height. The normal distribution only fits the data and does not generate it, which is why other data is created here.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "In order to reproduce the experiment, the [code](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1099891) is still provided here, in order to carry out the experiment itself with a group:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [123,456,789] # List of all heights from your examination group\n",
    "plt.hist(heights, bins='auto')  # arguments are passed to np.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Used functions and keywords (and etc.): [dictionary](https://docs.python.org/3/tutorial/datastructures.html), [.hist()](https://plot.ly/matplotlib/histograms/)</div>\n",
    "\n",
    "\n",
    "# Introduction to AR(1) Processes\n",
    "\n",
    "\n",
    "The Auto Regression Process model is a statistical model to represent a random process. Thereby, this model might fit well for the evolution of some variables over time. But it does **NOT** imply that the real-life data is generated by this model. To estimate this model, the correct way is (typically) to use OLS. \n",
    "The formula of the model is: \n",
    "![Bildschirmfoto%202018-03-27%20um%2017.57.30.png](attachment:Bildschirmfoto%202018-03-27%20um%2017.57.30.png)\n",
    "\n",
    "The line (4) shows the main formula. Thereby, $x_t$ is the value of the investigated dependend variable, $c$ is a constant variable, which describes $x_t$, $px_t$$_-$$_1$ is a fraction of the value of the dependend variable in the prior period and $e_t$ is a randomised value. The line (5) formulates the rule that the standard error is normal distributed. The line (6) formulates the rule that p has to be smaller than 1. \n",
    "\n",
    "**Example for the AR model**:\n",
    "The GDP of countries can be forecasted by using the AR model. The chart below shows the real GDP per capita values in a country and the dotted line shows the GDP per capita forecasted. \n",
    "\n",
    "![Bildschirmfoto%202018-03-27%20um%2018.25.29.png](attachment:Bildschirmfoto%202018-03-27%20um%2018.25.29.png)\n",
    "\n",
    "You can see that the forecast is pretty accurate. However, as in the human height example, it should be noted that this model only matches the GDP data. However, it is not the model that generates the GDP per capita driven by data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetition of last week's homework\n",
    "\n",
    "In the following, exercise five and six are solved and explained through #comments on the side of each line. Related to the topic of this lecture it is shown how data can be generated model-based (Exercise 6) and how they can be displayed (Exercise 5).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> **Exercise 5**: \n",
    "The files [06_Apple.txt](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1078952), [06_Microsoft.txt](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1078958), and [06_Tesla.txt](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1078950) contain fake data on the stock market value of these three companies for everyday of the last year. Load these data into Python and plot the time series in one plot.\n",
    "</div>\n",
    "\n",
    "The following [code](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1078954) loads the data of ```06_Apple.txt```, ```06_Microsoft.txt```, and ```06_Tesla.tx``` into Python and plots the time series in one plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports data from txt files and plots data\n",
    "import matplotlib.pyplot as plt # Imports data from matplotlib.pyplot and creates an alias (plt) for it\n",
    "import numpy as np # Imports data from numpy and creates an alias (np) for it\n",
    "\n",
    "# Lists the firms Apple, Microsoft and Tesla\n",
    "firms = [\"Apple\",\"Microsoft\",\"Tesla\"] \n",
    "\n",
    "\n",
    "for f in firms: # Iterates over the three firms\n",
    "    myname = \"06_\" + f + \".txt\" # myname is \"06_\" + f + \".txt\"\n",
    "    data = np.loadtxt(myname) # Loads data from the textfiles (myname)\n",
    "    plt.plot(data, label = f) # Plots a plot with the firms data \n",
    "\n",
    "# plots the legend in the plot\n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Used functions and keywords (and etc.): [import](https://www.programiz.com/python-programming/keyword-list#from_import), [matplotlib.pyplot](https://matplotlib.org/users/pyplot_tutorial.html), [numpy](https://wiki.python.org/moin/NumPy), [as](https://www.programiz.com/python-programming/keyword-list#as), [dictionary](https://docs.python.org/3/tutorial/datastructures.html), [for loop](https://wiki.python.org/moin/ForLoop), [.plt() and .show()](https://stackoverflow.com/questions/8575062/how-to-show-matplotlib-plots-in-python), [.legend()](https://matplotlib.org/users/legend_guide.html)</div>\n",
    "\n",
    "The plot shows the time series of ```06_Apple.txt```, ```06_Microsoft.txt```, and ```06_Tesla.tx```. Still, it is possible to build an even more efficient code, because it is known that we have a folder that contains different text files. You do not need to know the structure of these, but there is also a function that retrieves them more efficiently. However, this is one level further and will therefore not be deepened too much in the course.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> **Exercise 6**:\n",
    "The fake data has been created using an autoregressive process of order 1 (AR(1)):\n",
    "xt+1 = αxt + εt+1, (1)\n",
    "where x0 = 0, 0 ≤ α < 1 is the persistence parameter, and εt is the innovation shock which is a draw from a standard normal (i.i.d.). Choose three values for the persistence parameter α (one for each company), generate fake data for 365 days, and output these data in three different text files.\n",
    "</div>\n",
    "\n",
    "The following [code](https://fronter.com/unisg/links/link.phtml?idesc=1&iid=1078954) chose three values for a, generated fake data for 365 days and output this data in three different text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates realizations of AR(1) processes and prints them into a file\n",
    "import numpy as np # Imports data from numpy and creates an alias (np) for it\n",
    "\n",
    "# Set seed for innovation\n",
    "np.random.seed = (0) # Sets the random seed to 0; It generates each time the same billions of billion numbers\n",
    "\n",
    "α = {'Microsoft': 0.0, 'Apple': 0.5, 'Tesla': 0.99} # Used a list as a dictionary for alpha\n",
    "ts_length = 365 # ts_length is 365\n",
    "\n",
    "\n",
    "for firm in α: # Iterates over the three firms\n",
    "    # Open file where to store realisations\n",
    "    myname = \"06_\" + firm + \".txt\" # myname is \"06_\" + firm + \".txt\"\n",
    "    myfile = open(myname, 'w') # myfile is open myname in writing mode\n",
    "    \n",
    "    # Initialise current value\n",
    "    current_x = 0 # The current_x is zero\n",
    "    for i in range(ts_length): # Iterates over i in the range of ts_length\n",
    "        current_x = (α[firm] * current_x) + np.random.randn() # current_x is the a of the firm multipled with the current_x summed with np.random.randn()\n",
    "        myfile.write(\"%s\\n\" % current_x) # write myfile\n",
    "    myfile.close() # close it and free up any system resources taken up by the open file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the break we looked at the exercise 5 and therefore, we will solve exercise 6 and 7 in this session. Hence, let us start with the exercise 6. \n",
    "\n",
    "\n",
    "**Task**:\n",
    "\n",
    "The fake data has been created using an autoregressive process of order 1 (AR(1)):\n",
    "\n",
    "$x_{t+1}$ = $\\alpha$$x_{t}$ + $\\epsilon_{t+1}$\n",
    "\n",
    "where $x_{0}$ = 0, 0 ≤ $\\alpha$ < 1 is the persistence parameter, and $\\epsilon_{t}$ is the innovation shock which is a draw from a standard normal (i.i.d.). Choose three values for the persistence parameter $\\alpha$ (one for each company), generate fake data for 365 days, and output these data in three different text files.\n",
    "\n",
    "\n",
    "\n",
    "Exercise 6 asks us to generate fake data according to an AR(1) Process and to save them in three different text files. The solution for this exercise is found in the file '07_AR1_Calibration'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
